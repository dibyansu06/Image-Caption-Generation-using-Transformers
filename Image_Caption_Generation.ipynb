{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0zR7Mt4-GkV"
      },
      "source": [
        "# Image Caption Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIoLN5z3-QhS"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS90YlL4Th9y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, Model, Input\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "import librosa as lb\n",
        "from scipy.io.wavfile import write\n",
        "from IPython.display import Audio\n",
        "from google.colab import files\n",
        "import locale\n",
        "import torch\n",
        "lb.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV2c4n9_BV7A"
      },
      "source": [
        "## DataSets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqEzQQIX9fUV"
      },
      "outputs": [],
      "source": [
        "# !pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "eVY1EnDT-Csp",
        "outputId": "499bb4f9-49e7-4086-98f9-af3841589c12"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c22f4e1a-d951-4411-b6ed-130cb3008d95\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c22f4e1a-d951-4411-b6ed-130cb3008d95\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"dibyansusingh\",\"key\":\"03cd58f549351985dfa2890bc9e1d6ce\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir0_6Njf-Mvd"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuoB8PND-SB9"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjoyogdP-YWC"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d adityajn105/flickr30k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q98oooWlBaDS"
      },
      "source": [
        "## Global Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBRqQEIOUR7M"
      },
      "outputs": [],
      "source": [
        "# Path to the images\n",
        "IMAGES_PATH = \"/content/Images\"\n",
        "\n",
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "FF_DIM = 512\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "NUM_HEADS = 8\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plQ_fAZzBdJ9"
      },
      "source": [
        "## Load and map the images with captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGyLH8qPWilb"
      },
      "outputs": [],
      "source": [
        "def load_caption_data(filename):\n",
        "\n",
        "  with open(filename) as caption_file:\n",
        "    caption_data = caption_file.readlines()\n",
        "    caption_mapping = {}\n",
        "    text_data = []\n",
        "    images_to_skip = set()\n",
        "\n",
        "    for lines in caption_data[1:]:\n",
        "      line = lines.rstrip(\"\\n\")\n",
        "      img_name, caption = line.split(\",\", 1)\n",
        "      img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
        "\n",
        "      tokens = caption.strip().split()\n",
        "      if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:\n",
        "          images_to_skip.add(img_name)\n",
        "          continue\n",
        "\n",
        "      if img_name.endswith(\"jpg\"):\n",
        "        caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "        text_data.append(caption)\n",
        "        if img_name in caption_mapping:\n",
        "          caption_mapping[img_name].append(caption)\n",
        "        else:\n",
        "          caption_mapping[img_name] = [caption]\n",
        "\n",
        "    for img_name in images_to_skip:\n",
        "          if img_name in caption_mapping:\n",
        "              del caption_mapping[img_name]\n",
        "\n",
        "    return caption_mapping, text_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3a52PAkXWje"
      },
      "outputs": [],
      "source": [
        "captions_mapping, text_data = load_caption_data(\"captions.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqTU5yM6XSWz"
      },
      "outputs": [],
      "source": [
        "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
        "    \"\"\"Split the captioning dataset into train and validation sets.\n",
        "\n",
        "    Args:\n",
        "        caption_data (dict): Dictionary containing the mapped caption data\n",
        "        train_size (float): Fraction of all the full dataset to use as training data\n",
        "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
        "\n",
        "    Returns:\n",
        "        Traning and validation datasets as two separated dicts\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get the list of all image names\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    # 2. Shuffle if necessary\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # 3. Split into training and validation sets\n",
        "    train_size = int(len(caption_data) * train_size)\n",
        "\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
        "    }\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
        "    }\n",
        "\n",
        "    # 4. Return the splits\n",
        "    return training_data, validation_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NehD3KnBXTSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0337cfa-62f9-4801-c93f-25b0e108b827"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21056, 5264)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_data, valid_data = train_val_split(captions_mapping)\n",
        "\n",
        "len(train_data), len(valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgJA5eLYZv8j"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "vectorization.adapt(text_data)\n",
        "\n",
        "# Data augmentation for image data\n",
        "image_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomContrast(0.3),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyjZ34XZSWHx"
      },
      "source": [
        "### TensorFlow dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcJGKrzrZ_ku"
      },
      "outputs": [],
      "source": [
        "def decode_and_resize(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_input(img_path, captions):\n",
        "    return decode_and_resize(img_path), vectorization(captions)\n",
        "\n",
        "\n",
        "def make_dataset(images, captions, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n",
        "    # dataset = dataset.shuffle(batch_size * 8)\n",
        "    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size).prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pass the list of images and the list of corresponding captions\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()), BATCH_SIZE)\n",
        "\n",
        "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()), BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv3kDUnYv_Gn"
      },
      "source": [
        "## CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI-w9V1Fv90B"
      },
      "outputs": [],
      "source": [
        "def get_cnn_model(feature_model):\n",
        "    base_model = feature_model(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\")\n",
        "\n",
        "    # We freeze our feature extractor\n",
        "    base_model.trainable = False\n",
        "\n",
        "    base_model_out = base_model.output\n",
        "\n",
        "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
        "\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "\n",
        "    return cnn_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrqVCsdywC8C"
      },
      "source": [
        "## Transformer Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3hPHQ5AaRL7"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoderBlock, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        inputs = self.layernorm_1(inputs)\n",
        "        inputs = self.dense_1(inputs)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=None,\n",
        "            training=training,\n",
        "        )\n",
        "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
        "        return out_1\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerEncoderBlock, self).get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'dense_dim': self.dense_dim,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_tokens = embedded_tokens * self.embed_scale\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'sequence_length': self.sequence_length,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoderBlock, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
        "\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n",
        "        )\n",
        "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
        "\n",
        "        self.dropout_1 = layers.Dropout(0.3)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
        "        inputs = self.embedding(inputs)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "        else:\n",
        "            combined_mask = causal_mask\n",
        "            padding_mask = None  # Set padding_mask to None if mask is not provided\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,  # This will be None if mask is not provided\n",
        "            training=training,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoderBlock, self).get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'ff_dim': self.ff_dim,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, cnn_model, encoder, decoder, num_captions_per_image=5, image_aug=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "        self.image_aug = image_aug\n",
        "\n",
        "    def call(self, inputs):\n",
        "        image_data, text_data = inputs\n",
        "\n",
        "        # Processing with CNN model\n",
        "        img_embed = self.cnn_model(image_data)\n",
        "\n",
        "        # Encoder\n",
        "        encoder_out = self.encoder(img_embed)\n",
        "\n",
        "        # Decoder\n",
        "        caption_pred = self.decoder(text_data, encoder_out)\n",
        "\n",
        "        return caption_pred\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ImageCaptioningModel, self).get_config()\n",
        "        config.update({\n",
        "            'num_captions_per_image': self.num_captions_per_image,\n",
        "            'image_aug': self.image_aug\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
        "        encoder_out = self.encoder(img_embed, training=training)\n",
        "        batch_seq_inp = batch_seq[:, :-1]\n",
        "        batch_seq_true = batch_seq[:, 1:]\n",
        "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
        "        batch_seq_pred = self.decoder(\n",
        "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "        return loss, acc\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        if self.image_aug:\n",
        "            batch_img = self.image_aug(batch_img)\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, acc = self._compute_caption_loss_and_acc(\n",
        "                    img_embed, batch_seq[:, i, :], training=True\n",
        "                )\n",
        "\n",
        "                # 3. Update loss and accuracy\n",
        "                batch_loss += loss\n",
        "                batch_acc += acc\n",
        "\n",
        "            # 4. Get the list of all the trainable weights\n",
        "            train_vars = (\n",
        "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            )\n",
        "\n",
        "            # 5. Get the gradients\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        "\n",
        "            # 6. Update the trainable weights\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "\n",
        "        # 7. Update the trackers\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        # 8. Return the loss and accuracy values\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            loss, acc = self._compute_caption_loss_and_acc(\n",
        "                img_embed, batch_seq[:, i, :], training=False\n",
        "            )\n",
        "\n",
        "            # 3. Update batch loss and batch accuracy\n",
        "            batch_loss += loss\n",
        "            batch_acc += acc\n",
        "\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "\n",
        "        # 4. Update the trackers\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        # 5. Return the loss and accuracy values\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We need to list our metrics here so the `reset_states()` can be\n",
        "        # called automatically.\n",
        "        return [self.loss_tracker, self.acc_tracker]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwtZflwW7iD0"
      },
      "source": [
        "### LR schedular etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNGqxajy6t-g"
      },
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=\"none\"\n",
        ")\n",
        "\n",
        "# EarlyStopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Learning Rate Scheduler for the optimizer\n",
        "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
        "        super().__init__()\n",
        "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        global_step = tf.cast(step, tf.float32)\n",
        "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
        "        warmup_progress = global_step / warmup_steps\n",
        "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
        "        return tf.cond(\n",
        "            global_step < warmup_steps,\n",
        "            lambda: warmup_learning_rate,\n",
        "            lambda: self.post_warmup_learning_rate,\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"post_warmup_learning_rate\": self.post_warmup_learning_rate,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "        }\n",
        "\n",
        "\n",
        "# Create a learning rate schedule\n",
        "num_train_steps = len(train_dataset) * EPOCHS\n",
        "num_warmup_steps = num_train_steps // 15\n",
        "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rcziaspVM6L"
      },
      "source": [
        "## Model Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paj5hzrJVRzz"
      },
      "outputs": [],
      "source": [
        "# Get cnn model\n",
        "cnn_model = get_cnn_model(tf.keras.applications.EfficientNetB0) # You can use different models here\n",
        "\n",
        "# Get Encoder block\n",
        "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=6)\n",
        "\n",
        "# Get Decoder Block\n",
        "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=6)\n",
        "\n",
        "# Create the model\n",
        "caption_model= ImageCaptioningModel(cnn_model=cnn_model,\n",
        "                                                     encoder=encoder,\n",
        "                                                     decoder=decoder,\n",
        "                                                     image_aug=image_augmentation)\n",
        "\n",
        "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\n",
        "\n",
        "# Changes the dir name according to the model name and dataset\n",
        "dir_name = \"caption_model\"\n",
        "# !mkdir {dir_name}\n",
        "\n",
        "# # This saves the history to the directory created above\n",
        "# merged_logger = CSVLogger(f\"/content/{dir_name}/history.log\")\n",
        "\n",
        "# Fit the model\n",
        "history_EfficientNetB7 = caption_model.fit(train_dataset,\n",
        "                                          epochs=30,\n",
        "                                          # steps_per_epoch=int(len(valid_dataset) * 0.10),\n",
        "                                          validation_data=valid_dataset,\n",
        "                                          validation_steps=int(len(valid_dataset) * 0.10),\n",
        "                                          callbacks=[early_stopping])\n",
        "\n",
        "# Saves the model to the directory created\n",
        "\n",
        "sample_image = tf.random.normal([1, *IMAGE_SIZE, 3])  # Adjust the shape as needed\n",
        "sample_text = tf.random.uniform([1, SEQ_LENGTH], maxval=VOCAB_SIZE, dtype=tf.int32)  # Adjust as needed\n",
        "\n",
        "# Pass the sample inputs through the model\n",
        "caption_model((sample_image, sample_text))\n",
        "\n",
        "# Now save your model\n",
        "caption_model.save(f\"/content/drive/MyDrive/caption_model\") # Change the model name accordingly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPaNCIbE6HwK"
      },
      "source": [
        "## Predicting on a model\n",
        "\n",
        "Check here how the model is predicting on the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu7X1D0EJvoB"
      },
      "outputs": [],
      "source": [
        "vocab = vectorization.get_vocabulary()\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
        "valid_images = list(valid_data.keys())\n",
        "\n",
        "\n",
        "def generate_caption(model, mapped_data, n, plot):\n",
        "    def plot_img(img):\n",
        "        # Read the image from the disk\n",
        "        img = decode_and_resize(img)\n",
        "        img = img.numpy().clip(0, 255).astype(np.uint8)\n",
        "        plt.imshow(img)\n",
        "        plt.axis(False)\n",
        "        plt.show()\n",
        "        return img\n",
        "\n",
        "    for i in range(n):\n",
        "        # Select a random image from the validation dataset\n",
        "        sample_img = np.random.choice(list(mapped_data.keys()))\n",
        "\n",
        "        # Display the image if plot is True\n",
        "        if plot:\n",
        "            image = plot_img(sample_img)\n",
        "\n",
        "        # Pass the image to the CNN\n",
        "        img = tf.expand_dims(image, 0)\n",
        "        img = model.cnn_model(img)\n",
        "\n",
        "        # Pass the image features to the Transformer encoder\n",
        "        encoded_img = model.encoder(img, training=False)\n",
        "\n",
        "        # Generate the caption using the Transformer decoder\n",
        "        decoded_caption = \"<start> \"\n",
        "        for i in range(max_decoded_sentence_length):\n",
        "\n",
        "            tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
        "            mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "\n",
        "            predictions = model.decoder(tokenized_caption, encoded_img, training=False, mask=mask)\n",
        "\n",
        "            sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "            sampled_token = index_lookup[sampled_token_index]\n",
        "\n",
        "            if sampled_token == \"<end>\":\n",
        "                break\n",
        "            decoded_caption += \" \" + sampled_token\n",
        "\n",
        "        decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
        "        decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
        "\n",
        "        if plot:\n",
        "            print(\"Predicted Caption: \", decoded_caption)\n",
        "\n",
        "        bleu_score = corpus_bleu([mapped_data[sample_img]], [decoded_caption])\n",
        "        print(f'BLEU Score: {bleu_score:.4f}')\n",
        "\n",
        "        meteor_s = meteor_score([mapped_data[sample_img]], [decoded_caption])\n",
        "        print(f'METEOR Score: {meteor_s:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdNqkxsLJjLp"
      },
      "outputs": [],
      "source": [
        "generate_caption(model, train_data, 5, True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}